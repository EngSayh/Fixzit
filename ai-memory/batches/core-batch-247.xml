
You are the "Fixzit Memory Builder" for category: "core".

You are given a batch of source files from the Fixzit codebase, wrapped in <file> tags
inside <batch_content>. Each <file> has a "path" attribute with the repository-relative
file path, and its contents are wrapped in CDATA.

YOUR TASK:
1. Read ALL files in <batch_content>.
2. For EACH file, extract architectural metadata using this schema:

[
  {
    "file": "repo-relative/path/to/file.ext",
    "category": "core",
    "summary": "One-sentence technical summary of what this file does.",
    "exports": ["ExportedFunctionOrClassName", "..."],
    "dependencies": ["ImportedModuleOrPath", "..."]
  }
]

RULES:
- Return ONLY a valid JSON array.
- NO markdown, NO backticks, NO comments, NO extra text.
- Include an entry for every file in this batch.
- If a file has no exports, use "exports": [].
- If a file has no imports, use "dependencies": [].

<batch_content>

<file path="scripts/fixzit-security-fixes.js">
<![CDATA[
#!/usr/bin/env node

/**
 * FIXZIT SOUQ Security Vulnerability Fixes
 * Addresses all critical and high security issues in project source code
 */

const fs = require("fs").promises;
const crypto = require("crypto");

const EMAIL_DOMAIN = process.env.EMAIL_DOMAIN || 'fixzit.co';

// Security configuration
const SECURITY_CONFIG = {
  // Environment variables for sensitive data
  ENV_TEMPLATE: `
# Security Configuration
NODE_ENV=production
JWT_SECRET=${crypto.randomBytes(64).toString("hex")}
JWT_REFRESH_SECRET=${crypto.randomBytes(64).toString("hex")}
DB_PASSWORD=${crypto.randomBytes(32).toString("hex")}
ADMIN_DEFAULT_PASSWORD=${crypto.randomBytes(16).toString("hex")}
ENCRYPTION_KEY=${crypto.randomBytes(32).toString("hex")}
SESSION_SECRET=${crypto.randomBytes(32).toString("hex")}

# CORS Configuration
CORS_ORIGIN=https://fixzit.co
CORS_CREDENTIALS=true

# API Keys (use real values in production)
GOOGLE_MAPS_API_KEY=your_google_maps_api_key_here
ZATCA_API_KEY=your_zatca_api_key_here
SMS_API_KEY=your_sms_api_key_here
EMAIL_API_KEY=your_email_api_key_here
`,

  // Secure CORS configuration
  CORS_CONFIG: `
const corsOptions = {
  origin: function (origin, callback) {
    const allowedOrigins = process.env.CORS_ORIGIN?.split(',') || ['http://localhost:3000'];
    if (!origin || allowedOrigins.indexOf(origin) !== -1) {
      callback(null, true);
    } else {
      callback(new Error('Not allowed by CORS'));
    }
  },
  credentials: process.env.CORS_CREDENTIALS === 'true',
  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
  allowedHeaders: ['Content-Type', 'Authorization', 'X-Requested-With'],
  exposedHeaders: ['X-Total-Count'],
  maxAge: 86400 // 24 hours
};
`,

  // Secure token storage
  SECURE_TOKEN_STORAGE: `
// Use httpOnly cookies instead of localStorage for tokens
class SecureTokenStorage {
  static setToken(res, token, refreshToken) {
    res.cookie('access_token', token, {
      httpOnly: true,
      secure: process.env.NODE_ENV === 'production',
      sameSite: 'strict',
      maxAge: 15 * 60 * 1000 // 15 minutes
    });
    
    res.cookie('refresh_token', refreshToken, {
      httpOnly: true,
      secure: process.env.NODE_ENV === 'production',
      sameSite: 'strict',
      maxAge: 7 * 24 * 60 * 60 * 1000 // 7 days
    });
  }
  
  static clearTokens(res) {
    res.clearCookie('access_token');
    res.clearCookie('refresh_token');
  }
}
`,

  // XSS prevention
  XSS_PREVENTION: `
// Sanitize HTML to prevent XSS
const DOMPurify = require('isomorphic-dompurify');

function sanitizeHTML(dirty) {
  return DOMPurify.sanitize(dirty, {
    ALLOWED_TAGS: ['b', 'i', 'em', 'strong', 'a', 'p', 'br'],
    ALLOWED_ATTR: ['href', 'title', 'target']
  });
}

// Safe element update without innerHTML
function safeUpdateElement(element, content) {
  // Clear existing content
  while (element.firstChild) {
    element.removeChild(element.firstChild);
  }
  
  // Add sanitized content
  const sanitized = sanitizeHTML(content);
  const temp = document.createElement('div');
  temp.innerHTML = sanitized;
  
  while (temp.firstChild) {
    element.appendChild(temp.firstChild);
  }
}
`,
};

// Fix functions for each file
async function fixDatabaseSeed() {
  const seedFile = `const bcrypt = require('bcrypt');
const { User, Property, WorkOrder, Tenant } = require('../models');
require('dotenv').config();

async function seedDatabase() {
  try {
    // Use environment variables for sensitive data
    const adminPassword = process.env.ADMIN_DEFAULT_PASSWORD || crypto.randomBytes(16).toString('hex');
    const hashedPassword = await bcrypt.hash(adminPassword, 12);
    
    // Create admin user
    const admin = await User.create({
      name: 'System Administrator',
      email: 'admin@${EMAIL_DOMAIN}',
      password: hashedPassword,
      role: 'admin',
      organization: 'Fixzit'
    });
    
    console.log('Admin user created. Password stored securely in environment variables.');
    
    // Create test users with hashed passwords
    const testUsers = [
      { name: 'Property Manager', email: 'manager@test.com', role: 'manager' },
      { name: 'Technician', email: 'tech@test.com', role: 'technician' },
      { name: 'Tenant', email: 'tenant@test.com', role: 'tenant' }
    ];
    
    for (const userData of testUsers) {
      const tempPassword = crypto.randomBytes(16).toString('hex');
      const hashed = await bcrypt.hash(tempPassword, 12);
      await User.create({
        ...userData,
        password: hashed,
        organization: 'Test Org'
      });
      console.log(\`Created \${userData.role} with secure password\`);
    }
    
    console.log('Database seeded successfully');
  } catch (error) {
    console.error('Seeding error:', error);
  }
}

module.exports = seedDatabase;`;

  await fs.writeFile("database/seed-fixed.js", seedFile);
  console.log("âœ… Fixed database/seed.js - removed hardcoded passwords");
}

async function fixServerJS() {
  const serverFile = `const express = require('express');
const cors = require('cors');
const helmet = require('helmet');
const rateLimit = require('express-rate-limit');
const mongoSanitize = require('express-mongo-sanitize');
require('dotenv').config();

const app = express();

// Security middleware
app.use(helmet({
  contentSecurityPolicy: {
    directives: {
      defaultSrc: ["'self'"],
      styleSrc: ["'self'", "'unsafe-inline'", "https://fonts.googleapis.com"],
      scriptSrc: ["'self'", "https://apis.google.com"],
      fontSrc: ["'self'", "https://fonts.gstatic.com"],
      imgSrc: ["'self'", "data:", "https:"],
      connectSrc: ["'self'", "https://api.fixzit.co"]
    }
  }
}));

// Rate limiting
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // limit each IP to 100 requests per windowMs
  message: 'Too many requests from this IP, please try again later.'
});
app.use('/api/', limiter);

// CORS with proper configuration
${SECURITY_CONFIG.CORS_CONFIG}
app.use(cors(corsOptions));

// Prevent MongoDB injection attacks
app.use(mongoSanitize());

// Body parsing with size limits
app.use(express.json({ limit: '10mb' }));
app.use(express.urlencoded({ extended: true, limit: '10mb' }));

// Cookie parser for secure token handling
app.use(require('cookie-parser')());

// Routes
app.use('/api/auth', require('./routes/auth'));
app.use('/api/users', require('./routes/users'));
app.use('/api/properties', require('./routes/properties'));
app.use('/api/workorders', require('./routes/workorders'));
app.use('/api/finance', require('./routes/finance'));

// Error handling
app.use((err, req, res, next) => {
  console.error(err.stack);
  // Don't leak error details in production
  const message = process.env.NODE_ENV === 'production' 
    ? 'Internal Server Error' 
    : err.message;
  res.status(err.status || 500).json({ error: message });
});

const PORT = process.env.PORT || 5000;
app.listen(PORT, () => {
  console.log(\`Server running securely on port \${PORT}\`);
});`;

  await fs.writeFile("server-fixed.js", serverFile);
  console.log(
    "âœ… Fixed server.js - added proper CORS configuration and security middleware",
  );
}

async function fixPublicAppJS() {
  const appFile = `// Secure App.js with XSS prevention and secure token storage
${SECURITY_CONFIG.XSS_PREVENTION}

class FixzitApp {
  constructor() {
    this.init();
  }
  
  init() {
    // Use secure cookie-based authentication instead of localStorage
    this.checkAuthentication();
    this.setupEventListeners();
  }
  
  checkAuthentication() {
    // Check for httpOnly cookie presence via API call
    fetch('/api/auth/check', {
      credentials: 'include'
    })
    .then(res => res.json())
    .then(data => {
      if (data.authenticated) {
        this.loadDashboard();
      } else {
        this.showLogin();
      }
    });
  }
  
  updateDashboard(data) {
    const dashboard = document.getElementById('dashboard');
    if (!dashboard) return;
    
    // Safe update without innerHTML
    safeUpdateElement(dashboard, data.content);
  }
  
  displayWorkOrders(orders) {
    const container = document.getElementById('work-orders');
    if (!container) return;
    
    // Clear and rebuild safely
    container.innerHTML = ''; // Clear first
    
    orders.forEach(order => {
      const orderEl = document.createElement('div');
      orderEl.className = 'work-order';
      
      // Create elements safely
      const title = document.createElement('h3');
      title.textContent = order.title; // textContent is XSS-safe
      
      const description = document.createElement('p');
      description.textContent = order.description;
      
      const status = document.createElement('span');
      status.className = \`status \${order.status}\`;
      status.textContent = order.status;
      
      orderEl.appendChild(title);
      orderEl.appendChild(description);
      orderEl.appendChild(status);
      container.appendChild(orderEl);
    });
  }
  
  setupEventListeners() {
    // Prevent form-based XSS
    document.querySelectorAll('form').forEach(form => {
      form.addEventListener('submit', (e) => {
        const inputs = form.querySelectorAll('input, textarea');
        inputs.forEach(input => {
          // Sanitize input values
          if (input.type !== 'password') {
            input.value = DOMPurify.sanitize(input.value);
          }
        });
      });
    });
  }
}

// Initialize app
document.addEventListener('DOMContentLoaded', () => {
  new FixzitApp();
});`;

  await fs.writeFile("public/app-fixed.js", appFile);
  console.log(
    "âœ… Fixed public/app.js - removed localStorage token storage and innerHTML XSS vulnerabilities",
  );
}

async function createEnvExample() {
  await fs.writeFile(".env.example", SECURITY_CONFIG.ENV_TEMPLATE);
  console.log("âœ… Created .env.example with secure configuration template");
}

// Main execution
async function applySecurityFixes() {
  console.log("ðŸ”’ Applying FIXZIT SOUQ Security Fixes...\n");

  try {
    // Ensure directories exist
    await fs.mkdir("database", { recursive: true });
    await fs.mkdir("public", { recursive: true });

    // Apply fixes
    await fixDatabaseSeed();
    await fixServerJS();
    await fixPublicAppJS();
    await createEnvExample();

    console.log("\nðŸŽ‰ Security fixes applied successfully!");
    console.log("\nðŸ“‹ Next steps:");
    console.log(
      "1. Install security packages: npm install helmet express-rate-limit express-mongo-sanitize cookie-parser isomorphic-dompurify",
    );
    console.log("2. Create .env file: cp .env.example .env");
    console.log("3. Replace files with fixed versions");
    console.log("4. Restart your application");
  } catch (error) {
    console.error("âŒ Error applying security fixes:", error);
  }
}

if (require.main === module) {
  applySecurityFixes();
}

module.exports = { applySecurityFixes };

]]>
</file>

<file path="scripts/fixzit-server.js">
<![CDATA[
const express = require("express");
const path = require("path");
const app = express();
const PORT = process.env.PORT || 5000;

// Serve static files from public directory
app.use(express.static("public"));

// Route all requests to index.html for SPA behavior
app.get("/", (req, res) => {
  res.sendFile(path.join(__dirname, "public", "index.html"));
});

// Handle all other routes for SPA
app.use((req, res) => {
  res.sendFile(path.join(__dirname, "public", "index.html"));
});

app.listen(PORT, "0.0.0.0", () => {
  console.log(`ðŸš€ FIXZIT SOUQ running on port ${PORT}`);
  console.log(`âœ… Serving static files from public/ directory`);
  console.log(`ðŸŒ Access: http://localhost:${PORT}`);
});

]]>
</file>

<file path="scripts/fixzit-unified-audit-system.js">
<![CDATA[
/**
 * ========================================
 * FIXZIT SOUQ UNIFIED AUDIT SYSTEM v2.0
 * ========================================
 * COMPLETE CONSOLIDATED IMPLEMENTATION
 * Covers 100% of ALL THREE PLATFORMS
 * ========================================
 */

// ============================================
// PART 1: UNIFIED PLATFORM DEFINITION
// ============================================

class FacilityManagement {
  constructor() {
    this.modules = [
      "Dashboard", // 1
      "WorkOrders", // 2
      "Properties", // 3
      "Finance", // 4
      "HumanResources", // 5
      "Administration", // 6
      "CRM", // 7
      "Marketplace", // 8 (bridge to Souq)
      "Support", // 9
      "Compliance", // 10
      "Reports", // 11
      "SystemManagement", // 12
    ];

    this.workflows = {
      workOrderLifecycle: [
        "Intake",
        "Triage",
        "Dispatch",
        "Execute",
        "QC",
        "Close",
        "Bill",
      ],
      preventiveMaintenance: [
        "Schedule",
        "Generate",
        "Assign",
        "Execute",
        "Document",
      ],
    };
  }
}

class FixzitSouq {
  constructor() {
    this.modules = [
      "HomeDiscovery", // 1
      "Catalog", // 2
      "SearchFilters", // 3
      "RFQBidding", // 4
      "CartCheckout", // 5
      "VendorPortal", // 6
      "BuyerPortal", // 7
      "SupportDisputes", // 8
      "Analytics", // 9
      "Integrations", // 10
    ];

    this.workflows = {
      procurementCycle: [
        "RFQ",
        "Bids",
        "Compare",
        "Award",
        "Contract",
        "Order",
        "Fulfillment",
        "Payout",
      ],
    };
  }
}

class AqarSouq {
  constructor() {
    this.modules = [
      "HomeExplore", // 1
      "Listings", // 2
      "PostProperty", // 3
      "MapSearch", // 4
      "LeadsCRM", // 5
      "MortgageValuation", // 6
      "Projects", // 7
      "AgentDeveloperPortal", // 8
      "CommunityContent", // 9
      "SupportSafety", // 10
    ];

    this.workflows = {
      listingLifecycle: [
        "Post",
        "Moderation",
        "Publish",
        "Lead",
        "Appointment",
        "Offer",
        "Deal",
      ],
    };
  }
}

// ============================================
// PART 2: COMPLETE ROLE MATRIX (14 ROLES)
// ============================================

const UserRole = {
  SUPER_ADMIN: "SUPER_ADMIN", // 1
  OWNER_ADMIN: "OWNER_ADMIN", // 2
  MANAGEMENT: "MANAGEMENT", // 3
  FINANCE: "FINANCE", // 4
  HR: "HR", // 5
  OPERATIONS_DISPATCHER: "OPERATIONS", // 6
  TECHNICIAN: "TECHNICIAN", // 7
  VENDOR: "VENDOR", // 8
  CUSTOMER_TENANT: "CUSTOMER", // 9
  PROPERTY_OWNER: "PROPERTY_OWNER", // 10
  CRM_SALES: "CRM_SALES", // 11
  SUPPORT_AGENT: "SUPPORT_AGENT", // 12
  CORPORATE_EMPLOYEE: "CORPORATE_EMPLOYEE", // 13
  VIEWER_GUEST: "VIEWER_GUEST", // 14
};

const COMPLETE_ROLE_MATRIX = [
  {
    role: UserRole.SUPER_ADMIN,
    fmAccess: ["*"],
    souqAccess: ["*"],
    aqarAccess: ["*"],
    doaLimit: Infinity,
    crossPlatform: true,
  },
  {
    role: UserRole.OWNER_ADMIN,
    fmAccess: ["all_modules", "doa", "billing", "users"],
    souqAccess: ["org_setup", "buyer_approvals"],
    aqarAccess: ["agency_admin", "packages"],
    doaLimit: 1000000,
    crossPlatform: true,
  },
  {
    role: UserRole.OPERATIONS_DISPATCHER,
    fmAccess: ["work_orders", "dispatch", "pm"],
    souqAccess: ["buyer_rfqs"],
    aqarAccess: ["lead_management"],
    doaLimit: 50000,
    crossPlatform: true,
  },
  {
    role: UserRole.TECHNICIAN,
    fmAccess: ["my_work", "time_labor"],
    souqAccess: [],
    aqarAccess: [],
    doaLimit: 0,
    crossPlatform: false,
  },
  {
    role: UserRole.VENDOR,
    fmAccess: ["vendor_page"],
    souqAccess: ["listings", "orders", "payouts"],
    aqarAccess: ["valuation_partner", "mortgage_partner"],
    doaLimit: 0,
    crossPlatform: true,
  },
  {
    role: UserRole.CUSTOMER_TENANT,
    fmAccess: ["tickets", "approvals"],
    souqAccess: ["buyer_checkout", "rfqs"],
    aqarAccess: ["inquiry", "book_viewing"],
    doaLimit: 5000,
    crossPlatform: true,
  },
];

// ============================================
// PART 3: CROSS-PLATFORM BRIDGES
// ============================================

class CrossPlatformBridges {
  constructor() {
    this.bridges = [
      {
        from: "FM",
        to: "SOUQ",
        dataFlow:
          "RFQs published to marketplace; awarded bids create PO/Orders in FM",
        implementation: () => this.syncFMToSouq(),
      },
      {
        from: "SOUQ",
        to: "FM",
        dataFlow: "Service orders auto-create FM Work Orders with linked SLAs",
        implementation: () => this.syncSouqToFM(),
      },
      {
        from: "FM",
        to: "AQAR",
        dataFlow: "Property objects sync; Tenant leads flow to FM CRM",
        implementation: () => this.syncFMToAqar(),
      },
      {
        from: "AQAR",
        to: "FM",
        dataFlow: "Maintenance requests from tenant portal â†’ FM Tickets/WO",
        implementation: () => this.syncAqarToFM(),
      },
      {
        from: "AQAR",
        to: "SOUQ",
        dataFlow:
          "Source services (photography, staging) from listing workflow",
        implementation: () => this.syncAqarToSouq(),
      },
    ];
  }

  syncFMToSouq() {
    console.log("âœ… Syncing FM RFQs to Souq marketplace...");
    return true;
  }

  syncSouqToFM() {
    console.log("âœ… Creating FM Work Orders from Souq service orders...");
    return true;
  }

  syncFMToAqar() {
    console.log("âœ… Syncing properties to Aqar listings...");
    return true;
  }

  syncAqarToFM() {
    console.log("âœ… Converting Aqar maintenance requests to FM tickets...");
    return true;
  }

  syncAqarToSouq() {
    console.log("âœ… Sourcing services for Aqar listings from Souq...");
    return true;
  }
}

// ============================================
// PART 4: UNIFIED AUDIT ENGINE
// ============================================

class MasterAuditSystem {
  constructor() {
    this.fm = new FacilityManagement();
    this.souq = new FixzitSouq();
    this.aqar = new AqarSouq();
    this.bridges = new CrossPlatformBridges();
    this.issues = new Map();
  }

  /**
   * COMPLETE SYSTEM AUDIT
   * Runs all checks across all platforms
   */
  async runCompleteAudit() {
    console.log("ðŸ” Starting COMPLETE FIXZIT ECOSYSTEM AUDIT...\n");

    const results = {
      timestamp: new Date(),
      platforms: {
        fm: await this.auditFM(),
        souq: await this.auditSouq(),
        aqar: await this.auditAqar(),
      },
      bridges: await this.auditBridges(),
      roles: await this.auditRoles(),
      technical: await this.auditTechnical(),
      database: await this.auditDatabase(),
      ui: await this.auditUI(),
      workflows: await this.auditWorkflows(),
      compliance: await this.auditCompliance(),
      issues: this.consolidateIssues(),
      score: this.calculateScore(),
    };

    this.printResults(results);
    return results;
  }

  // ========== PLATFORM AUDITS ==========

  async auditFM() {
    console.log("ðŸ“Š Auditing Facility Management Platform...");
    const results = [];

    for (const mod of this.fm.modules) {
      const moduleAudit = await this.auditModule("FM", mod);
      results.push(moduleAudit);

      if (moduleAudit.issues.length > 0) {
        this.issues.set(`FM.${mod}`, moduleAudit.issues);
      }
    }

    // Check FM-specific requirements
    const specificChecks = {
      preventiveMaintenance: this.checkPMScheduling(),
      dispatchMap: this.checkDispatchMap(),
      slaTracking: this.checkSLATracking(),
      doaApprovals: this.checkDoAWorkflow(),
    };

    return {
      platform: "FM",
      modulesTotal: 12,
      modulesPassed: results.filter((r) => r.status === "PASS").length,
      specificChecks,
      overallStatus: this.determineStatus(results),
    };
  }

  async auditSouq() {
    console.log("ðŸ›’ Auditing Fixzit Souq Platform...");
    const results = [];

    for (const mod of this.souq.modules) {
      const moduleAudit = await this.auditModule("SOUQ", mod);
      results.push(moduleAudit);

      if (moduleAudit.issues.length > 0) {
        this.issues.set(`SOUQ.${mod}`, moduleAudit.issues);
      }
    }

    // Check Souq-specific requirements
    const specificChecks = {
      amazonStyleGrid: this.checkAmazonGrid(),
      rfqWorkflow: this.checkRFQWorkflow(),
      multiVendorCart: this.checkMultiVendorCart(),
      vendorScoring: this.checkVendorScoring(),
    };

    return {
      platform: "SOUQ",
      modulesTotal: 10,
      modulesPassed: results.filter((r) => r.status === "PASS").length,
      specificChecks,
      overallStatus: this.determineStatus(results),
    };
  }

  async auditAqar() {
    console.log("ðŸ  Auditing Aqar Souq Platform...");
    const results = [];

    for (const mod of this.aqar.modules) {
      const moduleAudit = await this.auditModule("AQAR", mod);
      results.push(moduleAudit);

      if (moduleAudit.issues.length > 0) {
        this.issues.set(`AQAR.${mod}`, moduleAudit.issues);
      }
    }

    // Check Aqar-specific requirements
    const specificChecks = {
      mapSearch: this.checkMapSearchClusters(),
      propertyWizard: this.checkPropertyPostWizard(),
      mortgageIntegration: this.checkMortgagePartners(),
      leadManagement: this.checkLeadCRM(),
    };

    return {
      platform: "AQAR",
      modulesTotal: 10,
      modulesPassed: results.filter((r) => r.status === "PASS").length,
      specificChecks,
      overallStatus: this.determineStatus(results),
    };
  }

  // ========== CROSS-PLATFORM AUDITS ==========

  async auditBridges() {
    console.log("ðŸŒ‰ Auditing Cross-Platform Bridges...");
    const results = [];

    for (const bridge of this.bridges.bridges) {
      const bridgeTest = await this.testBridge(bridge);
      results.push({
        bridge: `${bridge.from} â†’ ${bridge.to}`,
        dataFlow: bridge.dataFlow,
        status: bridgeTest.success ? "CONNECTED" : "BROKEN",
        latency: bridgeTest.latency,
        issues: bridgeTest.issues,
      });
    }

    return {
      totalBridges: 5,
      connectedBridges: results.filter((r) => r.status === "CONNECTED").length,
      results,
    };
  }

  async auditRoles() {
    console.log("ðŸ‘¥ Auditing Role Matrix (14 Roles)...");
    const results = [];

    for (const roleConfig of COMPLETE_ROLE_MATRIX) {
      const roleTest = await this.testRole(roleConfig);
      results.push({
        role: roleConfig.role,
        fmAccess: roleTest.fmAccess,
        souqAccess: roleTest.souqAccess,
        aqarAccess: roleTest.aqarAccess,
        doaLimit: roleConfig.doaLimit,
        crossPlatform: roleConfig.crossPlatform,
        status: roleTest.allPermissionsWork ? "PASS" : "FAIL",
      });
    }

    return {
      totalRoles: 14,
      rolesConfigured: results.filter((r) => r.status === "PASS").length,
      results,
    };
  }

  async auditTechnical() {
    console.log("âš™ï¸ Auditing Technical Requirements...");

    return {
      multiTenant: await this.checkMultiTenancy(),
      authentication: await this.checkAuth(),
      api: {
        rest: await this.checkRESTAPI(),
        graphql: await this.checkGraphQL(),
        websockets: await this.checkWebSockets(),
      },
      performance: {
        loadTime: await this.measureLoadTime(),
        apiLatency: await this.measureAPILatency(),
        dbQueries: await this.measureDBPerformance(),
      },
      security: await this.runSecurityScan(),
    };
  }

  async auditDatabase() {
    console.log("ðŸ’¾ Auditing Database...");

    return {
      mockData: this.checkMockDataUsage(),
      realDatabase: await this.checkRealDatabase(),
      migrations: await this.checkMigrations(),
      indexes: await this.checkIndexes(),
      connectionPool: await this.checkConnectionPool(),
      transactions: await this.checkTransactions(),
    };
  }

  async auditUI() {
    console.log("ðŸŽ¨ Auditing UI/UX Requirements...");

    return {
      colors: this.checkColorScheme(),
      landingPage: this.checkLandingPage(),
      sidebar: this.checkSidebarPattern(),
      tabs: this.checkTabsNotSubmenus(),
      quickCreate: this.checkQuickCreateMenu(),
      rtl: this.checkRTLSupport(),
      responsive: this.checkResponsive(),
    };
  }

  async auditWorkflows() {
    console.log("ðŸ”„ Auditing Critical Workflows...");

    return {
      fmWorkflow: await this.testWorkOrderFlow(),
      souqWorkflow: await this.testRFQFlow(),
      aqarWorkflow: await this.testListingFlow(),
    };
  }

  async auditCompliance() {
    console.log("ðŸ“‹ Auditing Compliance...");

    return {
      zatca: await this.checkZATCACompliance(),
      gdpr: await this.checkGDPRCompliance(),
      localization: {
        arabic: this.checkArabicTranslation(),
        hijriCalendar: this.checkHijriCalendar(),
      },
    };
  }

  // ========== HELPER METHODS ==========

  async auditModule(platform, module) {
    // Simulate module audit
    const checks = [
      this.checkModuleLoads(platform, module),
      this.checkModulePermissions(platform, module),
      this.checkModuleData(platform, module),
      this.checkModuleUI(platform, module),
    ];

    const results = await Promise.all(checks);
    const issues = results.filter((r) => !r.success).map((r) => r.issue);

    return {
      module,
      status: issues.length === 0 ? "PASS" : "FAIL",
      issues,
      timestamp: new Date(),
    };
  }

  async testBridge(bridge) {
    try {
      bridge.implementation();
      return {
        success: true,
        latency: Math.random() * 100,
        issues: [],
      };
    } catch (error) {
      return {
        success: false,
        latency: -1,
        issues: [error.message],
      };
    }
  }

  async testRole(roleConfig) {
    return {
      fmAccess: roleConfig.fmAccess.length > 0,
      souqAccess:
        roleConfig.souqAccess.length > 0 ||
        roleConfig.role === UserRole.TECHNICIAN,
      aqarAccess:
        roleConfig.aqarAccess.length > 0 ||
        roleConfig.role === UserRole.TECHNICIAN,
      allPermissionsWork: true,
    };
  }

  consolidateIssues() {
    let critical = 0,
      high = 0,
      medium = 0,
      low = 0;

    for (const [, issues] of this.issues) {
      for (const issue of issues) {
        switch (issue.severity) {
          case "CRITICAL":
            critical++;
            break;
          case "HIGH":
            high++;
            break;
          case "MEDIUM":
            medium++;
            break;
          case "LOW":
            low++;
            break;
        }
      }
    }

    return {
      critical,
      high,
      medium,
      low,
      total: critical + high + medium + low,
    };
  }

  calculateScore() {
    return {
      overall: 92,
      breakdown: {
        fm: 95,
        souq: 90,
        aqar: 88,
        bridges: 95,
        technical: 93,
      },
    };
  }

  // ========== MOCK CHECK IMPLEMENTATIONS ==========

  checkPMScheduling() {
    return { status: "PASS", details: "PM scheduling active" };
  }
  checkDispatchMap() {
    return { status: "PASS", details: "Dispatch map functional" };
  }
  checkSLATracking() {
    return { status: "PASS", details: "SLA tracking enabled" };
  }
  checkDoAWorkflow() {
    return { status: "PASS", details: "DoA matrix enforced" };
  }
  checkAmazonGrid() {
    return { status: "PASS", details: "Amazon-style grid implemented" };
  }
  checkRFQWorkflow() {
    return { status: "PASS", details: "RFQ workflow complete" };
  }
  checkMultiVendorCart() {
    return { status: "PASS", details: "Multi-vendor cart working" };
  }
  checkVendorScoring() {
    return { status: "PASS", details: "Vendor scoring active" };
  }
  checkMapSearchClusters() {
    return { status: "PASS", details: "Map clusters working" };
  }
  checkPropertyPostWizard() {
    return { status: "PASS", details: "Property wizard complete" };
  }
  checkMortgagePartners() {
    return { status: "PASS", details: "Mortgage partners integrated" };
  }
  checkLeadCRM() {
    return { status: "PASS", details: "Lead CRM functional" };
  }

  async checkMultiTenancy() {
    return true;
  }
  async checkAuth() {
    return true;
  }
  async checkRESTAPI() {
    return true;
  }
  async checkGraphQL() {
    return true;
  }
  async checkWebSockets() {
    return true;
  }
  async measureLoadTime() {
    return 1200;
  }
  async measureAPILatency() {
    return 85;
  }
  async measureDBPerformance() {
    return 35;
  }
  async runSecurityScan() {
    return { vulnerabilities: 0 };
  }

  checkMockDataUsage() {
    return false;
  }
  async checkRealDatabase() {
    return true;
  }
  async checkMigrations() {
    return true;
  }
  async checkIndexes() {
    return true;
  }
  async checkConnectionPool() {
    return true;
  }
  async checkTransactions() {
    return true;
  }

  checkColorScheme() {
    return true;
  }
  checkLandingPage() {
    return true;
  }
  checkSidebarPattern() {
    return true;
  }
  checkTabsNotSubmenus() {
    return true;
  }
  checkQuickCreateMenu() {
    return true;
  }
  checkRTLSupport() {
    return true;
  }
  checkResponsive() {
    return true;
  }

  async testWorkOrderFlow() {
    return { status: "PASS" };
  }
  async testRFQFlow() {
    return { status: "PASS" };
  }
  async testListingFlow() {
    return { status: "PASS" };
  }

  async checkZATCACompliance() {
    return { status: "PASS", details: "ZATCA ready" };
  }
  async checkGDPRCompliance() {
    return { status: "PASS" };
  }
  checkArabicTranslation() {
    return true;
  }
  checkHijriCalendar() {
    return true;
  }

  checkModuleLoads(_platform, _module) {
    return Promise.resolve({ success: true, issue: null });
  }
  checkModulePermissions(_platform, _module) {
    return Promise.resolve({ success: true, issue: null });
  }
  checkModuleData(_platform, _module) {
    return Promise.resolve({ success: true, issue: null });
  }
  checkModuleUI(_platform, _module) {
    return Promise.resolve({ success: true, issue: null });
  }

  determineStatus(results) {
    return results.every((r) => r.status === "PASS") ? "PASS" : "FAIL";
  }

  printResults(results) {
    console.log("\n==================================================");
    console.log("ðŸŽ¯ FIXZIT ECOSYSTEM AUDIT RESULTS");
    console.log("==================================================");

    console.log("\nðŸ“Š PLATFORM SCORES:");
    console.log(`   FM (Facility Management): ${results.score.breakdown.fm}%`);
    console.log(`   SOUQ (Marketplace): ${results.score.breakdown.souq}%`);
    console.log(`   AQAR (Real Estate): ${results.score.breakdown.aqar}%`);

    console.log("\nðŸŒ‰ CROSS-PLATFORM BRIDGES:");
    console.log(
      `   Connected: ${results.bridges.connectedBridges}/${results.bridges.totalBridges}`,
    );

    console.log("\nðŸ‘¥ ROLE MATRIX:");
    console.log(
      `   Configured: ${results.roles.rolesConfigured}/${results.roles.totalRoles} roles`,
    );

    console.log("\nâš¡ PERFORMANCE:");
    console.log(`   Load Time: ${results.technical.performance.loadTime}ms`);
    console.log(
      `   API Latency: ${results.technical.performance.apiLatency}ms`,
    );
    console.log(
      `   DB Performance: ${results.technical.performance.dbQueries}ms`,
    );

    console.log("\nðŸŽ¨ UI/UX COMPLIANCE:");
    console.log(`   Color Scheme: âœ… Fixzit Brand Colors`);
    console.log(`   Landing Page: âœ… 3-Button Layout`);
    console.log(`   RTL Support: âœ… Arabic Ready`);

    console.log("\nðŸ“‹ COMPLIANCE STATUS:");
    console.log(`   ZATCA: âœ… ${results.compliance.zatca.status}`);
    console.log(`   GDPR: âœ… ${results.compliance.gdpr.status}`);

    console.log(`\nðŸ† OVERALL SCORE: ${results.score.overall}%`);
    console.log("==================================================\n");
  }
}

// ============================================
// EXECUTE AUDIT
// ============================================

async function runFullAudit() {
  const auditor = new MasterAuditSystem();
  const results = await auditor.runCompleteAudit();

  console.log("âœ… Audit completed successfully!");
  console.log(`ðŸ“Š Full results available in audit object`);

  return results;
}

// Run the audit
runFullAudit().catch(console.error);

]]>
</file>

<file path="scripts/fixzit_all_in_one.py">
<![CDATA[
# scripts/fixzit_all_in_one.py
"""
Fixzit - Comprehensive Verification & Optimization System
=========================================================
Implements complete system verification, optimization, and maintenance.
"""

from __future__ import annotations
import os
import sys
import json
import subprocess
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple, Set

# ---------------------------- Paths ----------------------------
ROOT = Path(__file__).resolve().parents[1]
ART = ROOT / "artifacts"
SHOT = ART / "screenshots"
BACKUPS = ART / "backups"
PAGES = ROOT / "pages"
MODULES = ROOT / "modules"

# Create directories
ART.mkdir(exist_ok=True)
SHOT.mkdir(exist_ok=True)
BACKUPS.mkdir(exist_ok=True)

# ---------------------------- Utils ----------------------------
def _print(s: str): 
    sys.stdout.write(s + "\n")
    sys.stdout.flush()

def _w(p: Path, t: str): 
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(t, encoding="utf-8")

def timestamp() -> str:
    import datetime
    return datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

def run(cmd: List[str] | str, label: str, autofix=False) -> Tuple[int, str]:
    _print(f"\n=== {label} ===")
    if isinstance(cmd, str):
        proc = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    else:
        proc = subprocess.run(cmd, capture_output=True, text=True)
    
    out = (proc.stdout or "") + (proc.stderr or "")
    
    if proc.returncode == 0:
        _print(f"[OK] {label}")
    else:
        _print(f"[FAIL] {label} (rc={proc.returncode})")
        if autofix:
            try:
                if "ruff" in str(cmd):
                    subprocess.run([sys.executable, "-m", "ruff", "check", ".", "--fix"])
                if "black" in str(cmd):
                    subprocess.run([sys.executable, "-m", "black", "."])
            except Exception:
                pass
    
    return proc.returncode, out

# ---------------------------- Main Verifications ----------------------------
def verify_navigation():
    """Verify navigation system is working correctly"""
    _print("\n=== Verifying Navigation System ===")
    issues = []
    
    # Check navigation_new.py exists
    nav_file = ROOT / "navigation_new.py"
    if not nav_file.exists():
        issues.append("navigation_new.py is missing")
        return issues
    
    # Check for proper imports
    content = nav_file.read_text(encoding="utf-8")
    required_imports = ["streamlit", "typing", "base64"]
    for imp in required_imports:
        if f"import {imp}" not in content:
            issues.append(f"Missing import: {imp}")
    
    # Check NAV structure is properly defined
    if "NAV: List[Dict]" not in content:
        issues.append("NAV structure not properly typed")
    
    # Check for all navigation groups
    required_groups = ["Dashboard", "Work Management", "Properties", "Finance", 
                      "Marketplace", "Users & Teams", "Administration", 
                      "Communications", "Security & Sharing", "System Tools"]
    for group in required_groups:
        if f'group": "{group}"' not in content:
            issues.append(f"Missing navigation group: {group}")
    
    return issues

def verify_modules():
    """Verify all modules exist and are properly structured"""
    _print("\n=== Verifying Modules ===")
    issues = []
    
    # Check modules directory exists
    if not MODULES.exists():
        issues.append("modules/ directory is missing")
        return issues
    
    # Get list of module files
    module_files = list(MODULES.glob("*.py"))
    if len(module_files) < 60:
        issues.append(f"Expected 60+ modules, found only {len(module_files)}")
    
    # Check critical modules exist
    critical_modules = [
        "01_Dashboard_WorkOS.py",
        "00_Login.py",
        "03_Tickets_WorkOS.py",
        "05_Properties_WorkOS.py",
        "09_Payments_WorkOS.py",
        "23_User_Management_WorkOS.py",
        "31_Admin_Panel.py"
    ]
    
    for module in critical_modules:
        module_path = MODULES / module
        if not module_path.exists():
            issues.append(f"Critical module missing: {module}")
        else:
            # Check module has basic Streamlit structure
            content = module_path.read_text(encoding="utf-8")
            if "import streamlit as st" not in content:
                issues.append(f"Module {module} missing Streamlit import")
    
    return issues

def verify_database():
    """Verify database connectivity and structure"""
    _print("\n=== Verifying Database ===")
    issues = []
    
    try:
        import psycopg2
        
        # Check DATABASE_URL is set
        db_url = os.environ.get("DATABASE_URL")
        if not db_url:
            issues.append("DATABASE_URL environment variable not set")
            return issues
        
        # Try to connect
        try:
            conn = psycopg2.connect(db_url)
            conn.autocommit = True
            cur = conn.cursor()
            
            # Check critical tables exist
            critical_tables = ["users", "properties", "contracts", "tickets", "payments"]
            cur.execute("""
                SELECT table_name FROM information_schema.tables 
                WHERE table_schema = 'public'
            """)
            existing_tables = [row[0] for row in cur.fetchall()]
            
            for table in critical_tables:
                if table not in existing_tables:
                    issues.append(f"Critical table missing: {table}")
            
            conn.close()
        except Exception as e:
            issues.append(f"Database connection failed: {str(e)}")
    
    except ImportError:
        issues.append("psycopg2 not installed")
    
    return issues

def verify_streamlit_config():
    """Verify Streamlit configuration"""
    _print("\n=== Verifying Streamlit Config ===")
    issues = []
    
    # Check .streamlit/config.toml exists
    config_path = ROOT / ".streamlit" / "config.toml"
    if not config_path.exists():
        issues.append(".streamlit/config.toml is missing")
        return issues
    
    # Check required settings
    content = config_path.read_text(encoding="utf-8")
    required_settings = [
        'port = 5000',
        'address = "0.0.0.0"',
        'headless = true'
    ]
    
    for setting in required_settings:
        if setting not in content:
            issues.append(f"Missing config setting: {setting}")
    
    return issues

def verify_app_structure():
    """Verify main app.py structure"""
    _print("\n=== Verifying App Structure ===")
    issues = []
    
    # Check app.py exists
    app_file = ROOT / "app.py"
    if not app_file.exists():
        issues.append("app.py is missing")
        return issues
    
    content = app_file.read_text(encoding="utf-8")
    
    # Check required imports and setup
    required_elements = [
        "import streamlit as st",
        "from navigation_new import render_sidebar",
        "st.set_page_config",
        "render_sidebar("
    ]
    
    for element in required_elements:
        if element not in content:
            issues.append(f"app.py missing: {element}")
    
    return issues

def fix_issues(all_issues: Dict[str, List[str]]):
    """Attempt to fix identified issues"""
    _print("\n=== Attempting Auto-Fixes ===")
    fixes_applied = []
    
    # Fix Streamlit config if missing
    if "Streamlit Config" in all_issues:
        config_path = ROOT / ".streamlit" / "config.toml"
        config_path.parent.mkdir(exist_ok=True)
        config_content = """[server]
headless = true
address = "0.0.0.0"
port = 5000

[theme]
primaryColor = "#F6851F"
backgroundColor = "#FFFFFF"
secondaryBackgroundColor = "#F0F2F6"
textColor = "#262730"
"""
        config_path.write_text(config_content, encoding="utf-8")
        fixes_applied.append("Created .streamlit/config.toml")
    
    return fixes_applied

def generate_report(all_issues: Dict[str, List[str]], fixes: List[str]):
    """Generate verification report"""
    report = ["# Fixzit System Verification Report", ""]
    report.append(f"Generated: {timestamp()}")
    report.append("")
    
    # Summary
    total_issues = sum(len(issues) for issues in all_issues.values())
    report.append(f"## Summary")
    report.append(f"- Total Issues Found: {total_issues}")
    report.append(f"- Auto-fixes Applied: {len(fixes)}")
    report.append("")
    
    # Issues by category
    report.append("## Issues by Category")
    for category, issues in all_issues.items():
        if issues:
            report.append(f"\n### {category}")
            for issue in issues:
                report.append(f"- âŒ {issue}")
        else:
            report.append(f"\n### {category}")
            report.append("- âœ… All checks passed")
    
    # Fixes applied
    if fixes:
        report.append("\n## Auto-fixes Applied")
        for fix in fixes:
            report.append(f"- âœ… {fix}")
    
    # Save report
    report_path = ART / "verification_report.md"
    report_path.write_text("\n".join(report), encoding="utf-8")
    _print(f"\nReport saved to: {report_path}")
    
    return "\n".join(report)

def main():
    """Main verification runner"""
    _print("=" * 60)
    _print("Fixzit System Verification & Optimization")
    _print("=" * 60)
    
    # Run all verifications
    all_issues = {
        "Navigation": verify_navigation(),
        "Modules": verify_modules(),
        "Database": verify_database(),
        "Streamlit Config": verify_streamlit_config(),
        "App Structure": verify_app_structure()
    }
    
    # Attempt fixes
    fixes = fix_issues(all_issues)
    
    # Generate report
    report = generate_report(all_issues, fixes)
    
    # Print summary
    total_issues = sum(len(issues) for issues in all_issues.values())
    _print("\n" + "=" * 60)
    if total_issues == 0:
        _print("âœ… ALL VERIFICATIONS PASSED!")
    else:
        _print(f"âš ï¸ {total_issues} issues found - see artifacts/verification_report.md")
    _print("=" * 60)
    
    return 0 if total_issues == 0 else 1

if __name__ == "__main__":
    sys.exit(main())
]]>
</file>

<file path="scripts/fixzit_quick_verify.py">
<![CDATA[
#!/usr/bin/env python3
"""
Fixzit Quick Verification Script
A focused verification that completes quickly
"""

import os
import sys
import subprocess
import json
from pathlib import Path
from datetime import datetime

# Setup paths
ROOT = Path(__file__).resolve().parents[1]
ARTIFACTS = ROOT / "artifacts"
ARTIFACTS.mkdir(exist_ok=True)


def print_header(title):
    print(f"\n{'='*60}")
    print(f"  {title}")
    print("=" * 60)


def run_check(cmd, label, timeout=10):
    """Run a command and return success status"""
    try:
        if isinstance(cmd, str):
            result = subprocess.run(
                cmd, shell=True, capture_output=True, text=True, timeout=timeout
            )
        else:
            result = subprocess.run(
                cmd, capture_output=True, text=True, timeout=timeout
            )

        if result.returncode == 0:
            print(f"âœ… {label}")
            return True, result.stdout
        else:
            print(f"âŒ {label}")
            return False, result.stderr
    except subprocess.TimeoutExpired:
        print(f"âš ï¸ {label} (timeout)")
        return False, "Timeout"
    except Exception as e:
        print(f"âš ï¸ {label}: {e}")
        return False, str(e)


def main():
    print("ðŸ”§ FIXZIT - Quick Verification Report")
    print(f"ðŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ðŸ“ Root: {ROOT}")

    results = {
        "timestamp": datetime.now().isoformat(),
        "checks": {},
        "summary": {"passed": 0, "failed": 0, "warnings": 0},
    }

    # 1. Core Files Check
    print_header("1. Core Files")
    core_files = ["app.py", "requirements.txt", "scripts/fixzit_review_all.py"]
    for file in core_files:
        path = ROOT / file
        if path.exists():
            print(f"âœ… {file} exists")
            results["checks"][file] = "exists"
            results["summary"]["passed"] += 1
        else:
            print(f"âŒ {file} missing")
            results["checks"][file] = "missing"
            results["summary"]["failed"] += 1

    # 2. Python Syntax Check
    print_header("2. Python Syntax")
    for py_file in ["app.py", "navigation.py", "nav_config.py"]:
        if (ROOT / py_file).exists():
            success, _ = run_check(
                f"python -m py_compile {py_file}", f"{py_file} syntax"
            )
            if success:
                results["summary"]["passed"] += 1
            else:
                results["summary"]["failed"] += 1

    # 3. Dependencies Check
    print_header("3. Dependencies")
    deps = [
        ("streamlit", "Core framework"),
        ("psycopg2", "Database driver"),
        ("black", "Code formatter"),
        ("ruff", "Linter"),
        ("mypy", "Type checker"),
    ]

    for module, desc in deps:
        cmd = f'python -c "import {module}"'
        success, _ = run_check(cmd, f"{module} ({desc})")
        if success:
            results["summary"]["passed"] += 1
        else:
            results["summary"]["failed"] += 1

    # 4. Application Status
    print_header("4. Application Status")

    # Check if app is running
    success, _ = run_check(
        'curl -s -o /dev/null -w "%{http_code}" http://localhost:5000 | grep -q "200"',
        "App accessible on port 5000",
    )
    if success:
        results["summary"]["passed"] += 1
        results["checks"]["app_status"] = "running"
    else:
        results["summary"]["failed"] += 1
        results["checks"]["app_status"] = "not accessible"

    # 5. Database Connection
    print_header("5. Database")

    if os.environ.get("DATABASE_URL"):
        print("âœ… DATABASE_URL is set")
        results["summary"]["passed"] += 1

        # Test connection
        test_script = """
import os
import psycopg2
try:
    conn = psycopg2.connect(os.environ["DATABASE_URL"])
    conn.close()
    print("Connected")
except Exception as e:
    print(f"Error: {e}")
"""
        success, output = run_check(
            f'python -c "{test_script}"', "Database connection test"
        )
        if success and "Connected" in output:
            results["summary"]["passed"] += 1
        else:
            results["summary"]["failed"] += 1
    else:
        print("âš ï¸ DATABASE_URL not set")
        results["summary"]["warnings"] += 1

    # 6. Code Quality (Quick)
    print_header("6. Code Quality (Quick)")

    # Black check
    success, _ = run_check(
        "python -m black --check app.py 2>/dev/null", "Black formatting"
    )
    if not success:
        print("  â„¹ï¸ Run 'black app.py' to fix formatting")
        results["summary"]["warnings"] += 1
    else:
        results["summary"]["passed"] += 1

    # Ruff check
    success, output = run_check("python -m ruff check app.py --quiet", "Ruff linting")
    if not success and output:
        print("  â„¹ï¸ Run 'ruff check app.py --fix' to fix issues")
        results["summary"]["warnings"] += 1
    else:
        results["summary"]["passed"] += 1

    # 7. Project Structure
    print_header("7. Project Structure")

    important_dirs = ["pages", "utils", "scripts", "artifacts"]
    for dir_name in important_dirs:
        dir_path = ROOT / dir_name
        if dir_path.exists():
            count = len(list(dir_path.glob("*")))
            print(f"âœ… {dir_name}/ ({count} items)")
            results["summary"]["passed"] += 1
        else:
            print(f"âš ï¸ {dir_name}/ missing")
            results["summary"]["warnings"] += 1

    # Final Summary
    print_header("SUMMARY")
    print(f"âœ… Passed: {results['summary']['passed']}")
    print(f"âŒ Failed: {results['summary']['failed']}")
    print(f"âš ï¸ Warnings: {results['summary']['warnings']}")

    total = sum(results["summary"].values())
    if total > 0:
        success_rate = (results["summary"]["passed"] / total) * 100
        print(f"ðŸŽ¯ Success Rate: {success_rate:.1f}%")

    # Save results
    report_path = (
        ARTIFACTS / f"quick-verify-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"
    )
    with open(report_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nðŸ“„ Report saved to: {report_path}")

    # Exit code
    if results["summary"]["failed"] > 0:
        print("\nâŒ Verification completed with failures")
        return 1
    elif results["summary"]["warnings"] > 0:
        print("\nâš ï¸ Verification completed with warnings")
        return 0
    else:
        print("\nâœ… All checks passed!")
        return 0


if __name__ == "__main__":
    sys.exit(main())

]]>
</file>

<file path="scripts/fixzit_review_all.py">
<![CDATA[
"""
Fixzit â€” One-File, Zero-Error, Waterâ€‘Tight Verification & Repair Orchestrator
==============================================================================

Adds a strict **Instruction History Audit**:
- Scans DEV_INSTRUCTIONS.md, root *.md, docs/**, instructions/**, chat_history/**.
- Reads @require:... tags and instructions/requirements.json.
- Verifies files, functions, pages, UI text, DB tables/columns/FKs.
- Outputs a Traceability Matrix; any failure triggers a restart until ZERO errors.

Also enforces:
- Code quality (black, ruff, mypy, pytest [+cov optional]), dep health, pip-audit, bandit
- Secrets scan, requirements pinning, license review
- Duplication & redundancy (modules + functions) with auto-merge delegates
- Dead-code purge (unreachable modules â†’ moved to artifacts/trash-*)
- Dev instruction compliance with safe auto-stubs (onboarding, monitoring, sidebar)
- DB integrity + relationship auto-fix (add FKs + indexes, VALIDATE)
- DB roundtrip, backup â†’ JSONL, restore test to TEMP tables
- Mock + demo seeding for testing
- UI review (errors, a11y heuristics, dead links, perf) + UI interactions (sidebar, buttons, forms)
- Auth flows, API error handling, monitoring, onboarding
- ZEROâ€‘ERROR target with restart from the beginning after any fix/failure

Key artifacts:
- artifacts/instruction-matrix.(json|md) â€” Traceability Matrix (your instruction coverage)
- artifacts/final-report.md             â€” consolidated report
- artifacts/review-summary.(json|md)    â€” per-pass summary (pending errors, fixed all?)
"""

from __future__ import annotations
import os
import sys
import json
import re
import time
import subprocess
from pathlib import Path
from typing import Any, Dict, List, Tuple

ROOT = Path(__file__).resolve().parents[1]
ART = ROOT / "artifacts"
SHOT = ART / "screenshots"
BACKUPS = ART / "backups"
TRASH = ART / f"trash-{int(time.time())}"
PAGES = ROOT / "pages"
ART.mkdir(exist_ok=True)
SHOT.mkdir(exist_ok=True)
BACKUPS.mkdir(exist_ok=True)


# ---------------------------- utils ----------------------------
def _print(s: str):
    sys.stdout.write(s + "\n")
    sys.stdout.flush()


def _w(p: Path, t: str):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(t, encoding="utf-8")


def _jsonout(p: Path, d: Any):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(d, indent=2), encoding="utf-8")


def _logfile(label: str) -> Path:
    return ART / f"{label.replace(' ','_').lower()}.log"


def _strict() -> bool:
    return os.environ.get("FIXZIT_STRICT", "1").lower() not in ("0", "false", "no")


def _max_passes() -> int:
    try:
        return int(os.environ.get("FIXZIT_MAX_PASSES", "20"))
    except Exception:
        return 20


def timestamp() -> str:
    import datetime as _dt

    return _dt.datetime.now().strftime("%Y%m%d-%H%M%S")


def run(cmd: List[str] | str, label: str, autofix=False) -> Tuple[int, str]:
    _print(f"\n=== {label} ===")
    if isinstance(cmd, str):
        proc = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    else:
        proc = subprocess.run(cmd, capture_output=True, text=True)
    out = (proc.stdout or "") + (proc.stderr or "")
    if proc.returncode == 0:
        _print(f"[OK] {label}")
    else:
        _print(f"[FAIL] {label} (rc={proc.returncode})")
        _w(_logfile(label), out)
        if autofix:
            try:
                if "ruff" in str(cmd):
                    subprocess.run(
                        [sys.executable, "-m", "ruff", "check", ".", "--fix"]
                    )
                if "black" in str(cmd):
                    subprocess.run([sys.executable, "-m", "black", "."])
            except Exception:
                pass
    return proc.returncode, out


def ensure_import(mod: str, pkg: str | None = None) -> bool:
    try:
        __import__(mod)
        return True
    except Exception:
        name = pkg or mod
        _print(f"[info] pip install {name}")
        rc, _ = run(
            [sys.executable, "-m", "pip", "install", name], f"pip install {name}"
        )
        return rc == 0


def index_error(label: str, out: str):
    idx = ART / "error-index.md"
    lines = (
        ["# Error Index\n"]
        if not idx.exists()
        else idx.read_text(encoding="utf-8").splitlines()
    )
    count = sum(1 for line in lines if line.startswith("[ERR-"))
    eid = count + 1
    excerpt = (out or label).strip().replace("\r", "")[:1200]
    lines.append(f"[ERR-{eid:03d}] {label}: {excerpt}")
    _w(idx, "\n".join(lines))


def fail(label: str, out: str) -> bool:
    index_error(label, out)
    return False


def entry_candidates() -> List[Path]:
    return [
        ROOT / "Hello.py",
        ROOT / "app.py",
        ROOT / "streamlit_app.py",
        ROOT / "main.py",
    ]


def find_entry() -> Path:
    for p in entry_candidates():
        if p.exists():
            return p
    raise FileNotFoundError(
        "No Streamlit entry found (Hello.py/app.py/streamlit_app.py/main.py)"
    )


# ---------------------------- bootstrap tools ----------------------------
def step_bootstrap_tools() -> bool:
    ok = True
    for m, p in [
        ("requests", "requests"),
        ("psutil", "psutil"),
        ("psycopg2", "psycopg2-binary"),
        ("playwright", "playwright"),
        ("rich", "rich"),
        ("bandit", "bandit"),
        ("pip_audit", "pip-audit"),
        ("piplicenses", "pip-licenses"),
    ]:
        ok &= ensure_import(m, p)
    try:
        run(
            [sys.executable, "-m", "playwright", "install", "chromium"],
            "playwright install chromium",
        )
    except Exception as e:
        return fail("Bootstrap Playwright", str(e))
    return ok


# ======================================================================
# INSTRUCTION HISTORY AUDIT (Traceability Matrix)
# ======================================================================
REQ_GLOBS = [
    "DEV_INSTRUCTIONS.md",
    "*.md",
    "docs/**/*.md",
    "instructions/**/*.md",
    "chat_history/**/*.md",
]
REQ_JSON = Path("instructions/requirements.json")


def _glob_many(patterns: List[str]) -> List[Path]:
    out: List[Path] = []
    for pat in patterns:
        out.extend(list(ROOT.glob(pat)))
    # unique/keep existing files only
    return [
        p
        for i, p in enumerate(out)
        if p.exists() and p.as_posix() not in set(x.as_posix() for x in out[:i])
    ]


def collect_requirements() -> List[Dict[str, Any]]:
    reqs: List[Dict[str, Any]] = []
    # 1) JSON schema (preferred)
    if REQ_JSON.exists():
        try:
            data = json.loads(REQ_JSON.read_text(encoding="utf-8"))
            if isinstance(data, dict) and isinstance(data.get("requirements"), list):
                for item in data["requirements"]:
                    if isinstance(item, dict):
                        reqs.append(item)
        except Exception as e:
            reqs.append(
                {"id": "_json_parse_error", "type": "internal", "error": str(e)}
            )

    # 2) Markdown with @require: tags
    for md in _glob_many(REQ_GLOBS):
        try:
            text = md.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        # @require:file_exists=path
        for m in re.finditer(r"@require:file_exists=([^\s]+)", text, re.IGNORECASE):
            reqs.append(
                {"id": f"file:{m.group(1)}", "type": "file_exists", "value": m.group(1)}
            )
        for m in re.finditer(r"@require:page_exists=([^\n\r]+)", text, re.IGNORECASE):
            reqs.append(
                {
                    "id": f"page:{m.group(1).strip()}",
                    "type": "page_exists",
                    "value": m.group(1).strip(),
                }
            )
        for m in re.finditer(r'@require:ui_text="([^"]+)"', text, re.IGNORECASE):
            reqs.append(
                {"id": f"ui_text:{m.group(1)}", "type": "ui_text", "value": m.group(1)}
            )
        for m in re.finditer(
            r"@require:function=([A-Za-z0-9_\.]+):([A-Za-z0-9_]+)", text, re.IGNORECASE
        ):
            reqs.append(
                {
                    "id": f"fn:{m.group(1)}:{m.group(2)}",
                    "type": "function_exists",
                    "module": m.group(1),
                    "name": m.group(2),
                }
            )
        for m in re.finditer(
            r"@require:db_table=([A-Za-z0-9_]+)\.([A-Za-z0-9_]+)", text, re.IGNORECASE
        ):
            reqs.append(
                {
                    "id": f"tbl:{m.group(1)}.{m.group(2)}",
                    "type": "db_table_exists",
                    "schema": m.group(1),
                    "table": m.group(2),
                }
            )
        for m in re.finditer(
            r"@require:db_column=([A-Za-z0-9_]+)\.([A-Za-z0-9_]+)\.([A-Za-z0-9_]+)",
            text,
            re.IGNORECASE,
        ):
            reqs.append(
                {
                    "id": f"col:{m.group(1)}.{m.group(2)}.{m.group(3)}",
                    "type": "db_column_exists",
                    "schema": m.group(1),
                    "table": m.group(2),
                    "column": m.group(3),
                }
            )
        for m in re.finditer(
            r"@require:db_fk=([A-Za-z0-9_]+)\.([A-Za-z0-9_]+)", text, re.IGNORECASE
        ):
            # interpret as table.column (schema assumed public)
            reqs.append(
                {
                    "id": f"fk:public.{m.group(1)}.{m.group(2)}",
                    "type": "db_fk_exists",
                    "schema": "public",
                    "table": m.group(1),
                    "column": m.group(2),
                }
            )

    # Always include core expectations (so the audit still runs even if no tags are present)
    defaults = [
        {"id": "onboarding", "type": "page_exists", "value": "Onboarding"},
        {"id": "monitoring", "type": "file_exists", "value": "usage_logger.py"},
        {"id": "sidebar", "type": "file_exists", "value": "Hello.py"},
    ]
    # avoid duplicates by id
    have_ids = {r.get("id") for r in reqs}
    for d in defaults:
        if d["id"] not in have_ids:
            reqs.append(d)
    return reqs


def check_file_exists(path: str) -> Tuple[bool, str]:
    return (ROOT / path).exists(), f"{path} exists"


def check_page_exists(name: str) -> Tuple[bool, str]:
    # page file exists or matches UI nav
    cand = [
        PAGES / f"{name}.py",
        PAGES / f"{name.lower()}.py",
        PAGES / f"{name.title()}.py",
    ]
    if any(p.exists() for p in cand):
        return True, "page file present"
    return False, "page file not found"


def check_function_exists(module: str, name: str) -> Tuple[bool, str]:
    mod_path = ROOT / Path(module.replace(".", "/") + ".py")
    if not mod_path.exists():
        return False, f"module file not found: {mod_path}"
    try:
        import ast

        tree = ast.parse(mod_path.read_text(encoding="utf-8"))
        for n in tree.body:
            if isinstance(n, ast.FunctionDef) and n.name == name:
                return True, "function found"
        return False, "function not found"
    except Exception as e:
        return False, f"parse error: {e}"


def check_db_table(schema: str, table: str) -> Tuple[bool, str]:
    url = os.environ.get("DATABASE_URL")
    if not url:
        return False, "DATABASE_URL not set"
    if not ensure_import("psycopg2", "psycopg2-binary"):
        return False, "psycopg2 missing"
    import psycopg2

    try:
        con = psycopg2.connect(url)
        con.autocommit = True
        cur = con.cursor()
        cur.execute(
            """SELECT 1 FROM information_schema.tables WHERE table_schema=%s AND table_name=%s""",
            (schema, table),
        )
        ok = bool(cur.fetchone())
        cur.close()
        con.close()
        return ok, "exists" if ok else "not found"
    except Exception as e:
        return False, str(e)


def step_instruction_history_audit() -> bool:
    """Builds a Traceability Matrix of your instructions and verifies each one."""
    label = "Instruction History Audit"
    reqs = collect_requirements()
    results = []
    failures = 0

    for r in reqs:
        rtype = r.get("type", "").lower()
        rid = r.get("id") or f"{rtype}:{r.get('value')}"
        status = False
        evidence = ""
        try:
            if rtype == "file_exists":
                status, evidence = check_file_exists(r["value"])
            elif rtype == "page_exists":
                status, evidence = check_page_exists(r["value"])
            elif rtype == "function_exists":
                status, evidence = check_function_exists(r["module"], r["name"])
            elif rtype == "db_table_exists":
                status, evidence = check_db_table(r.get("schema", "public"), r["table"])
            else:
                status, evidence = (True, "unrecognized type (ignored)")
        except Exception as e:
            status, evidence = (False, f"error: {e}")

        results.append(
            {
                "id": rid,
                "type": rtype,
                "status": "PASS" if status else "FAIL",
                "evidence": evidence,
            }
        )
        if not status:
            failures += 1

    _jsonout(ART / "instruction-matrix.json", {"results": results})
    # MD table
    md = ["# Instruction History Audit (Traceability Matrix)", ""]
    md.append(f"**Total Requirements:** {len(results)}")
    md.append(f"**Passed:** {len(results) - failures}")
    md.append(f"**Failed:** {failures}")
    md.append("")
    md.append("| ID | Type | Status | Evidence |")
    md.append("|---|---|---|---|")
    for r in results:
        status_icon = "âœ…" if r["status"] == "PASS" else "âŒ"
        md.append(
            f"| {r['id']} | {r['type']} | {status_icon} {r['status']} | {r['evidence']} |"
        )
    _w(ART / "instruction-matrix.md", "\n".join(md))

    if failures == 0:
        _print(f"[OK] {label} â€” All {len(results)} requirements PASS")
        return True
    else:
        _print(
            f"[FAIL] {label} â€” {failures} failures out of {len(results)} requirements"
        )
        return False


# ======================================================================
# AUTO-STUB MISSING REQUIREMENTS
# ======================================================================
def step_auto_stub_missing() -> bool:
    """Creates safe stubs for missing files/pages when possible."""
    _print("\n=== Auto-Stub Missing Requirements ===")

    # Get failed requirements from the matrix
    matrix_file = ART / "instruction-matrix.json"
    if not matrix_file.exists():
        _print("[SKIP] No instruction matrix found, running audit first")
        step_instruction_history_audit()
        if not matrix_file.exists():
            return True

    try:
        matrix = json.loads(matrix_file.read_text(encoding="utf-8"))
        failed = [r for r in matrix.get("results", []) if r["status"] == "FAIL"]

        created = 0
        for req in failed:
            req_type = req.get("type")
            req_id = req.get("id", "")

            if req_type == "file_exists" and "usage_logger.py" in req_id:
                _create_usage_logger()
                created += 1
            elif req_type == "page_exists" and "onboarding" in req_id.lower():
                _create_onboarding_page()
                created += 1

        if created > 0:
            _print(f"[OK] Created {created} missing stubs")
        else:
            _print("[OK] No missing stubs to create")

        return True

    except Exception as e:
        _print(f"[FAIL] Auto-stub error: {e}")
        return False


def _create_usage_logger():
    """Create a basic usage logger stub"""
    path = ROOT / "usage_logger.py"
    if path.exists():
        return

    content = '''"""
Usage Logger - Basic monitoring and analytics
"""
import os
import json
import time
from datetime import datetime
from pathlib import Path

class UsageLogger:
    def __init__(self):
        self.log_file = Path("artifacts/usage.log")
        self.log_file.parent.mkdir(exist_ok=True)
    
    def log_event(self, event_type: str, data: dict = None):
        """Log a usage event"""
        entry = {
            "timestamp": datetime.now().isoformat(),
            "event_type": event_type,
            "data": data or {}
        }
        
        try:
            with open(self.log_file, "a") as f:
                f.write(json.dumps(entry) + "\\n")
        except Exception as e:
            print(f"Logging error: {e}")
    
    def log_page_view(self, page_name: str, user_id: str = None):
        """Log page view"""
        self.log_event("page_view", {
            "page": page_name,
            "user_id": user_id
        })
    
    def log_action(self, action: str, details: dict = None):
        """Log user action"""
        self.log_event("user_action", {
            "action": action,
            "details": details or {}
        })

# Global instance
logger = UsageLogger()
'''
    path.write_text(content)
    _print("[OK] Created usage_logger.py")


def _create_onboarding_page():
    """Create an onboarding page stub"""
    path = PAGES / "Onboarding.py"
    if path.exists():
        return

    content = '''"""
User Onboarding - Welcome and setup flow
"""
import streamlit as st
from navigation import render_sidebar
from auth import require_auth

st.set_page_config(
    page_title="Welcome to Fixzit",
    page_icon="ðŸ‘‹",
    layout="wide"
)

# Authentication
user = require_auth()
if not user:
    st.error("Please log in to continue")
    st.stop()

# Render navigation
tenant, route = render_sidebar(auth_ok=True, user_roles={user.get('role', 'tenant')})

st.title("ðŸ‘‹ Welcome to Fixzit!")
st.markdown("### Your comprehensive property management platform")

st.markdown("""
Welcome to Fixzit! We're excited to help you manage your properties efficiently.

**Key Features:**
- ðŸ¢ Property Management
- ðŸŽ« Maintenance Tickets  
- ðŸ’° Payment Processing
- ðŸ“„ Contract Management
- ðŸ‘¥ Multi-role Access

**Getting Started:**
1. Complete your profile information
2. Add your first property
3. Explore the dashboard
4. Submit your first maintenance request

Need help? Contact our support team anytime!
""")

col1, col2, col3 = st.columns(3)

with col1:
    if st.button("ðŸ“‹ Complete Profile", use_container_width=True):
        st.switch_page("pages/01_Dashboard_WorkOS.py")

with col2:
    if st.button("ðŸ¢ Add Property", use_container_width=True):
        st.switch_page("pages/05_Properties_WorkOS.py")

with col3:
    if st.button("ðŸ“Š View Dashboard", use_container_width=True):
        st.switch_page("pages/01_Dashboard_WorkOS.py")
'''
    path.write_text(content)
    _print("[OK] Created Onboarding.py page")


# ======================================================================
# MAIN ORCHESTRATOR
# ======================================================================
def main():
    """Zero-error orchestrator with instruction history audit and auto-repair loop."""
    _print("=" * 80)
    _print(
        "Fixzit â€” Comprehensive Verification & Repair with Instruction History Audit"
    )
    _print("=" * 80)

    max_passes = _max_passes()
    _strict()

    for pass_num in range(1, max_passes + 1):
        _print(f"\nðŸ”„ **PASS {pass_num}/{max_passes}**")

        # Step 1: Bootstrap tools
        if not step_bootstrap_tools():
            _print(f"[FAIL] Bootstrap failed on pass {pass_num}")
            if pass_num == max_passes:
                return 1
            continue

        # Step 2: Instruction History Audit (before fixes)
        step_instruction_history_audit()

        # Step 3: Auto-stub missing requirements
        step_auto_stub_missing()

        # Step 4: Re-audit after auto-stub
        final_audit_passed = step_instruction_history_audit()

        # Step 5: Basic quality checks
        basic_checks = []
        basic_checks.append(
            run(
                [sys.executable, "-m", "black", ".", "--check"],
                "Format Check (black)",
                autofix=True,
            )
        )
        basic_checks.append(
            run(
                [sys.executable, "-m", "ruff", "check", "."],
                "Lint Check (ruff)",
                autofix=True,
            )
        )

        all_passed = final_audit_passed and all(rc == 0 for rc, _ in basic_checks)

        # Generate pass summary
        summary = {
            "pass_number": pass_num,
            "max_passes": max_passes,
            "instruction_audit_passed": final_audit_passed,
            "basic_checks_passed": all(rc == 0 for rc, _ in basic_checks),
            "overall_status": "PASS" if all_passed else "FAIL",
        }

        _jsonout(ART / f"pass-{pass_num:02d}-summary.json", summary)

        if all_passed:
            _print(f"âœ… **PASS {pass_num} SUCCESSFUL - All checks passed!**")
            _w(
                ART / "final-report.md",
                f"""# Fixzit Verification Complete

**Status:** âœ… SUCCESS  
**Pass:** {pass_num}/{max_passes}  
**Timestamp:** {timestamp()}

## Summary
- âœ… Instruction History Audit: All requirements verified
- âœ… Code quality: Formatting and linting passed  
- âœ… Database: Connected and healthy
- âœ… Auto-stub: Missing requirements created

## Key Artifacts
- `artifacts/instruction-matrix.md` - Full traceability matrix
- `artifacts/final-report.md` - This summary report
- `artifacts/backups/` - Repository and database backups

Your Fixzit application is verified and ready for deployment!
""",
            )
            return 0
        else:
            _print(f"âŒ **PASS {pass_num} FAILED - Retrying...**")
            if pass_num == max_passes:
                _print(f"âŒ **MAXIMUM PASSES REACHED ({max_passes}) - GIVING UP**")
                return 1

            # Brief pause before next iteration
            time.sleep(1)

    return 1


if __name__ == "__main__":
    sys.exit(main())

]]>
</file>

<file path="scripts/fixzit_verify.py">
<![CDATA[
#!/usr/bin/env python3
"""
Fixzit Application Health Check & Verification
Comprehensive system validation for Streamlit app
"""
import os
import sys
import json
from pathlib import Path
from datetime import datetime
import traceback

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Terminal colors
PASS = "\033[92m"
FAIL = "\033[91m"
WARN = "\033[93m"
INFO = "\033[94m"
BOLD = "\033[1m"
END = "\033[0m"


class FixzitVerifier:
    def __init__(self):
        self.errors = []
        self.warnings = []
        self.passes = []
        self.stats = {"total_checks": 0, "passed": 0, "failed": 0, "warnings": 0}

    def log_pass(self, msg):
        print(f"{PASS}âœ“ {msg}{END}")
        self.passes.append(msg)
        self.stats["passed"] += 1

    def log_fail(self, msg):
        print(f"{FAIL}âœ— {msg}{END}")
        self.errors.append(msg)
        self.stats["failed"] += 1

    def log_warn(self, msg):
        print(f"{WARN}âš  {msg}{END}")
        self.warnings.append(msg)
        self.stats["warnings"] += 1

    def log_info(self, msg):
        print(f"{INFO}â„¹ {msg}{END}")

    def section(self, title):
        print(f"\n{BOLD}{'='*60}{END}")
        print(f"{BOLD}{title}{END}")
        print(f"{BOLD}{'='*60}{END}")

    def check_environment(self):
        """Check environment variables and database"""
        self.section("ENVIRONMENT & DATABASE CHECK")

        # Check for database URL
        db_url = os.environ.get("DATABASE_URL")
        if db_url:
            self.log_pass("DATABASE_URL environment variable found")
        else:
            self.log_warn("DATABASE_URL not found in environment")

        # Try database connection
        try:
            from utils.database import get_db_connection

            conn = get_db_connection()
            if conn:
                cursor = conn.cursor()

                # Count tables
                cursor.execute(
                    """
                    SELECT COUNT(*) FROM information_schema.tables 
                    WHERE table_schema = 'public'
                """
                )
                result = cursor.fetchone()
                table_count = result[0] if result else 0
                self.log_pass(f"Database connected: {table_count} tables found")

                # Check critical tables
                critical_tables = [
                    "users",
                    "properties",
                    "contracts",
                    "tickets",
                    "payments",
                ]
                for table in critical_tables:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    result = cursor.fetchone()
                    count = result[0] if result else 0
                    self.log_pass(f"Table '{table}': {count} records")

                cursor.close()
                conn.close()
                return True
            else:
                self.log_fail("Database connection failed")
                return False
        except Exception as e:
            self.log_fail(f"Database error: {str(e)}")
            return False

    def check_pages(self):
        """Verify all page files"""
        self.section("PAGE FILES CHECK")

        pages_dir = Path("pages")
        if not pages_dir.exists():
            self.log_fail("Pages directory not found")
            return False

        page_files = sorted(pages_dir.glob("*.py"))
        self.log_info(f"Found {len(page_files)} page files")

        workos_pages = []
        redirect_pages = []
        standalone_pages = []
        error_pages = []

        for page in page_files:
            try:
                content = page.read_text()

                # Categorize pages
                if "WorkOS" in page.name:
                    workos_pages.append(page.name)
                elif "st.switch_page" in content and len(content.split("\n")) < 20:
                    redirect_pages.append(page.name)
                else:
                    standalone_pages.append(page.name)

                # Check for common issues
                if "st.set_page_config" not in content and "Redirect" not in content:
                    self.log_warn(f"{page.name}: Missing page config")

                # Check for syntax
                compile(content, page.name, "exec")

            except SyntaxError as e:
                self.log_fail(f"{page.name}: Syntax error - {e}")
                error_pages.append(page.name)
            except Exception as e:
                self.log_warn(f"{page.name}: {e}")

        # Summary
        self.log_pass(f"WorkOS pages: {len(workos_pages)}")
        self.log_pass(f"Redirect pages: {len(redirect_pages)}")
        self.log_pass(f"Standalone pages: {len(standalone_pages)}")

        if error_pages:
            self.log_fail(f"Error pages: {error_pages}")
            return False

        return True

    def check_services(self):
        """Check service modules"""
        self.section("SERVICES CHECK")

        services_dir = Path("services")
        if not services_dir.exists():
            self.log_warn("Services directory not found")
            return True  # Not critical

        service_files = sorted(services_dir.glob("*.py"))
        self.log_info(f"Found {len(service_files)} service files")

        critical_services = [
            "i18n_service.py",
            "auth_service.py",
            "notification_service.py",
        ]

        for service in critical_services:
            service_path = services_dir / service
            if service_path.exists():
                try:
                    content = service_path.read_text()
                    compile(content, service, "exec")
                    self.log_pass(f"Critical service OK: {service}")
                except Exception as e:
                    self.log_fail(f"Critical service error: {service} - {e}")
            else:
                self.log_warn(f"Critical service missing: {service}")

        return True

    def check_navigation(self):
        """Check navigation structure"""
        self.section("NAVIGATION CHECK")

        try:
            # Check if main navigation exists
            from pathlib import Path

            nav_file = Path(__file__).parent.parent / "navigation.py"
            if nav_file.exists():
                self.log_pass("Main navigation file exists")
                # Count pages with navigation
                pages_dir = Path(__file__).parent.parent / "pages"
                nav_pages = list(pages_dir.glob("*.py"))
                self.log_pass(f"Found {len(nav_pages)} total pages")
            else:
                self.log_fail("Main navigation file not found")
                return False

            return True

        except Exception as e:
            self.log_fail(f"Navigation check failed: {e}")
            return False

    def check_dependencies(self):
        """Check Python package dependencies"""
        self.section("DEPENDENCIES CHECK")

        required = ["streamlit", "pandas", "psycopg2", "bcrypt", "plotly", "folium"]

        missing = []
        for package in required:
            try:
                __import__(package.replace("-", "_"))
                self.log_pass(f"Package installed: {package}")
            except ImportError:
                self.log_fail(f"Package missing: {package}")
                missing.append(package)

        if missing:
            self.log_info(f"Install missing packages: pip install {' '.join(missing)}")
            return False

        return True

    def check_lsp_errors(self):
        """Check for Language Server Protocol errors"""
        self.section("CODE QUALITY CHECK")

        # Check for common issues in key files
        issues = {
            "pages/05_Properties_WorkOS.py": 4,
            "pages/06_Contracts_WorkOS.py": 5,
            "pages/07_Tickets_WorkOS.py": 4,
            "pages/08_Payments_WorkOS.py": 3,
        }

        for file, count in issues.items():
            if Path(file).exists():
                self.log_warn(f"{file}: {count} LSP diagnostics")

        self.log_info("Run 'get_latest_lsp_diagnostics' for details")
        return True

    def generate_report(self):
        """Generate verification report"""
        self.section("REPORT GENERATION")

        # Create artifacts directory
        artifacts = Path("artifacts")
        artifacts.mkdir(exist_ok=True)

        # Generate JSON report
        report = {
            "timestamp": datetime.now().isoformat(),
            "stats": self.stats,
            "errors": self.errors,
            "warnings": self.warnings,
            "passes": self.passes,
            "status": "PASS" if not self.errors else "FAIL",
        }

        json_file = artifacts / "fixzit-verify.json"
        with open(json_file, "w") as f:
            json.dump(report, f, indent=2)
        self.log_pass(f"JSON report: {json_file}")

        # Generate Markdown report
        md_file = artifacts / "fixzit-verify.md"
        with open(md_file, "w") as f:
            f.write("# Fixzit Verification Report\n\n")
            f.write(
                f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            )
            f.write("## Summary\n\n")
            f.write(
                f"- **Status**: {'âœ… PASS' if report['status'] == 'PASS' else 'âŒ FAIL'}\n"
            )
            f.write(f"- **Passed**: {self.stats['passed']}\n")
            f.write(f"- **Failed**: {self.stats['failed']}\n")
            f.write(f"- **Warnings**: {self.stats['warnings']}\n\n")

            if self.errors:
                f.write("## Errors\n\n")
                for error in self.errors:
                    f.write(f"- âŒ {error}\n")
                f.write("\n")

            if self.warnings:
                f.write("## Warnings\n\n")
                for warning in self.warnings:
                    f.write(f"- âš ï¸ {warning}\n")
                f.write("\n")

            if self.passes:
                f.write("## Passed Checks\n\n")
                for passed in self.passes:
                    f.write(f"- âœ… {passed}\n")

        self.log_pass(f"Markdown report: {md_file}")

        return report


def main():
    verifier = FixzitVerifier()

    print(f"{BOLD}{'='*60}{END}")
    print(f"{BOLD}FIXZIT APPLICATION VERIFICATION{END}")
    print(f"{BOLD}{'='*60}{END}")
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # Run all checks
    checks = [
        verifier.check_dependencies,
        verifier.check_environment,
        verifier.check_pages,
        verifier.check_services,
        verifier.check_navigation,
        verifier.check_lsp_errors,
    ]

    for check in checks:
        try:
            verifier.stats["total_checks"] += 1
            check()
        except Exception as e:
            verifier.log_fail(f"Check failed: {e}")
            traceback.print_exc()

    # Generate report
    report = verifier.generate_report()

    # Final summary
    print(f"\n{BOLD}{'='*60}{END}")
    print(f"{BOLD}VERIFICATION COMPLETE{END}")
    print(f"{BOLD}{'='*60}{END}")

    if report["status"] == "PASS":
        print(f"{PASS}âœ… ALL CRITICAL CHECKS PASSED{END}")
    else:
        print(f"{FAIL}âŒ VERIFICATION FAILED - {len(verifier.errors)} ERRORS{END}")

    print("\nReports saved to: artifacts/fixzit-verify.(json|md)")

    # Exit with appropriate code
    sys.exit(0 if report["status"] == "PASS" else 1)


if __name__ == "__main__":
    main()

]]>
</file>

<file path="scripts/flatten-base-dictionaries.ts">
<![CDATA[
/**
 * Flatten monolithic TypeScript base dictionaries into modular JSON sources
 * This eliminates the 28k-line TS files that cause VS Code memory issues
 *
 * Usage: npx tsx scripts/flatten-base-dictionaries.ts
 */

import * as fs from "fs";
import * as path from "path";
import en from "../i18n/dictionaries/en";
import ar from "../i18n/dictionaries/ar";
import type {
  TranslationDictionary,
  TranslationBundle,
} from "../i18n/dictionaries/types";

const SOURCES_DIR = path.join(__dirname, "../i18n/sources");
const BACKUP_DIR = path.join(__dirname, "../i18n/dictionaries/backup");

// Ensure directories exist
if (!fs.existsSync(SOURCES_DIR)) {
  fs.mkdirSync(SOURCES_DIR, { recursive: true });
}
if (!fs.existsSync(BACKUP_DIR)) {
  fs.mkdirSync(BACKUP_DIR, { recursive: true });
}

interface FlattenStats {
  coercedNumbers: number;
  coercedBooleans: number;
  droppedKeys: string[];
}

/**
 * Flatten nested dictionary into dot-notation keys with schema validation
 */
function flattenDictionary(
  obj: TranslationDictionary,
  prefix = "",
  stats: FlattenStats = {
    coercedNumbers: 0,
    coercedBooleans: 0,
    droppedKeys: [],
  },
): { flattened: Record<string, string>; stats: FlattenStats } {
  const result: Record<string, string> = {};

  for (const [key, value] of Object.entries(obj)) {
    const newKey = prefix ? `${prefix}.${key}` : key;

    if (typeof value === "string") {
      result[newKey] = value;
    } else if (typeof value === "number") {
      console.warn(`âš ï¸  Coercing number to string: "${newKey}" = ${value}`);
      result[newKey] = String(value);
      stats.coercedNumbers++;
    } else if (typeof value === "boolean") {
      console.warn(`âš ï¸  Coercing boolean to string: "${newKey}" = ${value}`);
      result[newKey] = String(value);
      stats.coercedBooleans++;
    } else if (typeof value === "object" && value !== null) {
      const nested = flattenDictionary(
        value as TranslationDictionary,
        newKey,
        stats,
      );
      Object.assign(result, nested.flattened);
    } else {
      console.warn(
        `âš ï¸  Dropping non-string leaf: "${newKey}" (type: ${typeof value})`,
      );
      stats.droppedKeys.push(newKey);
    }
  }

  return { flattened: result, stats };
}

/**
 * Sanitize domain name to ensure filesystem safety
 */
function sanitizeDomain(domain: string): string {
  const DEFAULT_DOMAIN = "common";

  // Handle empty, whitespace-only, or dot-only segments
  if (!domain || domain.trim() === "" || domain === ".") {
    return DEFAULT_DOMAIN;
  }

  // Check for illegal characters (allow only alphanumeric, hyphen, underscore)
  const SAFE_DOMAIN_PATTERN = /^[a-z0-9_-]+$/i;
  if (!SAFE_DOMAIN_PATTERN.test(domain)) {
    console.warn(
      `âš ï¸  Unsafe domain name: "${domain}" - contains illegal characters, using "${DEFAULT_DOMAIN}"`,
    );
    return DEFAULT_DOMAIN;
  }

  // Check for path traversal attempts
  if (domain.includes("..") || domain.includes("/") || domain.includes("\\")) {
    console.warn(
      `âš ï¸  Path traversal attempt: "${domain}" - using "${DEFAULT_DOMAIN}"`,
    );
    return DEFAULT_DOMAIN;
  }

  return domain;
}

/**
 * Group flattened keys by top-level domain with validation
 */
function groupByDomain(
  translations: Record<string, string>,
): Map<string, Record<string, string>> {
  const domainMap = new Map<string, Record<string, string>>();
  const DEFAULT_DOMAIN = "common";

  for (const [key, value] of Object.entries(translations)) {
    // Extract domain (first segment before dot)
    const firstDot = key.indexOf(".");
    let rawDomain: string;

    if (firstDot <= 0) {
      // No dot or starts with dot - use default
      console.warn(`âš ï¸  Malformed key: "${key}" - using default domain`);
      rawDomain = DEFAULT_DOMAIN;
    } else {
      rawDomain = key.substring(0, firstDot);
    }

    const domain = sanitizeDomain(rawDomain);

    if (!domainMap.has(domain)) {
      domainMap.set(domain, {});
    }

    domainMap.get(domain)![key] = value;
  }

  return domainMap;
}

interface MergeStats {
  overriddenEnKeys: number;
  overriddenArKeys: number;
  preservedEnKeys: number;
  preservedArKeys: number;
}

/**
 * Merge with existing modular sources - fresh keys override stale entries
 */
function mergeWithExisting(
  domain: string,
  newEn: Record<string, string>,
  newAr: Record<string, string>,
): { bundle: TranslationBundle; stats: MergeStats } {
  const filePath = path.join(SOURCES_DIR, `${domain}.translations.json`);
  const stats: MergeStats = {
    overriddenEnKeys: 0,
    overriddenArKeys: 0,
    preservedEnKeys: 0,
    preservedArKeys: 0,
  };

  if (fs.existsSync(filePath)) {
    try {
      const existing: TranslationBundle = JSON.parse(
        fs.readFileSync(filePath, "utf-8"),
      );

      // Count keys that will be overridden
      for (const key of Object.keys(existing.en)) {
        if (key in newEn) {
          stats.overriddenEnKeys++;
        } else {
          stats.preservedEnKeys++;
        }
      }
      for (const key of Object.keys(existing.ar)) {
        if (key in newAr) {
          stats.overriddenArKeys++;
        } else {
          stats.preservedArKeys++;
        }
      }

      if (stats.overriddenEnKeys > 0 || stats.overriddenArKeys > 0) {
        console.warn(
          `  âš ï¸  ${domain}: Overriding ${stats.overriddenEnKeys} en, ${stats.overriddenArKeys} ar keys with fresh values`,
        );
      }

      return {
        bundle: {
          en: { ...existing.en, ...newEn }, // Fresh keys override
          ar: { ...existing.ar, ...newAr },
        },
        stats,
      };
    } catch (err) {
      console.warn(
        `âš ï¸  Could not merge existing ${domain}.translations.json:`,
        err,
      );
    }
  }

  return { bundle: { en: newEn, ar: newAr }, stats };
}

interface ParityIssue {
  domain: string;
  enCount: number;
  arCount: number;
  diff: number;
}

/**
 * Validate TranslationBundle integrity
 */
function validateBundle(domain: string, bundle: TranslationBundle): boolean {
  if (!bundle.en || typeof bundle.en !== "object") {
    console.error(`âŒ ${domain}: Missing or invalid 'en' object`);
    return false;
  }
  if (!bundle.ar || typeof bundle.ar !== "object") {
    console.error(`âŒ ${domain}: Missing or invalid 'ar' object`);
    return false;
  }

  // Check all values are strings
  for (const [key, value] of Object.entries(bundle.en)) {
    if (typeof value !== "string") {
      console.error(
        `âŒ ${domain}: Non-string value in en.${key} (type: ${typeof value})`,
      );
      return false;
    }
  }
  for (const [key, value] of Object.entries(bundle.ar)) {
    if (typeof value !== "string") {
      console.error(
        `âŒ ${domain}: Non-string value in ar.${key} (type: ${typeof value})`,
      );
      return false;
    }
  }

  return true;
}

/**
 * Write modular domain files with sorted keys for deterministic output
 */
function writeDomainFiles() {
  console.log("ðŸ“¦ Flattening base dictionaries...\n");

  const enResult = flattenDictionary(en);
  const arResult = flattenDictionary(ar);

  const enFlat = enResult.flattened;
  const arFlat = arResult.flattened;

  console.log(`  English: ${Object.keys(enFlat).length} keys`);
  console.log(`  Arabic:  ${Object.keys(arFlat).length} keys`);

  // Report schema issues
  if (
    enResult.stats.coercedNumbers > 0 ||
    enResult.stats.coercedBooleans > 0 ||
    enResult.stats.droppedKeys.length > 0
  ) {
    console.log(`\nâš ï¸  Schema Issues:`);
    if (enResult.stats.coercedNumbers > 0)
      console.log(
        `    - Coerced ${enResult.stats.coercedNumbers} numbers to strings`,
      );
    if (enResult.stats.coercedBooleans > 0)
      console.log(
        `    - Coerced ${enResult.stats.coercedBooleans} booleans to strings`,
      );
    if (enResult.stats.droppedKeys.length > 0)
      console.log(
        `    - Dropped ${enResult.stats.droppedKeys.length} non-string values`,
      );
  }
  console.log("");

  const enDomains = groupByDomain(enFlat);
  const arDomains = groupByDomain(arFlat);

  // Get all unique domains and sort for deterministic output
  const allDomains = Array.from(
    new Set([...enDomains.keys(), ...arDomains.keys()]),
  ).sort();

  console.log(`ðŸ“ Writing ${allDomains.length} domain files...\n`);

  let totalEnKeys = 0;
  let totalArKeys = 0;
  let totalOverriddenKeys = 0;
  const parityIssues: ParityIssue[] = [];

  // Backup existing files BEFORE writing (move backups before flatten run)
  console.log("ðŸ’¾ Creating backups of existing translation files...\n");
  const timestamp = Date.now();
  const enSource = path.join(__dirname, "../i18n/dictionaries/en.ts");
  const arSource = path.join(__dirname, "../i18n/dictionaries/ar.ts");

  if (fs.existsSync(enSource)) {
    const enStats = fs.statSync(enSource);
    if (enStats.size > 1000000) {
      // Only backup if > 1MB (huge file)
      const enBackup = path.join(BACKUP_DIR, `en.ts.backup.${timestamp}`);
      fs.copyFileSync(enSource, enBackup);
      console.log(
        `  âœ“ Backed up en.ts (${(enStats.size / 1024 / 1024).toFixed(1)} MB)`,
      );
    }
  }

  if (fs.existsSync(arSource)) {
    const arStats = fs.statSync(arSource);
    if (arStats.size > 1000000) {
      const arBackup = path.join(BACKUP_DIR, `ar.ts.backup.${timestamp}`);
      fs.copyFileSync(arSource, arBackup);
      console.log(
        `  âœ“ Backed up ar.ts (${(arStats.size / 1024 / 1024).toFixed(1)} MB)`,
      );
    }
  }

  // Prune old backups (keep last 5)
  const backupFiles = fs
    .readdirSync(BACKUP_DIR)
    .filter((f) => f.endsWith(".backup." + timestamp) === false)
    .sort()
    .reverse();

  if (backupFiles.length > 10) {
    console.log(`\nðŸ—‘ï¸  Pruning old backups (keeping last 10)...`);
    for (const file of backupFiles.slice(10)) {
      fs.unlinkSync(path.join(BACKUP_DIR, file));
      console.log(`  âœ“ Removed ${file}`);
    }
  }
  console.log("");

  for (const domain of allDomains) {
    const enTranslations = enDomains.get(domain) || {};
    const arTranslations = arDomains.get(domain) || {};

    // Merge with existing modular sources
    const { bundle, stats } = mergeWithExisting(
      domain,
      enTranslations,
      arTranslations,
    );

    totalOverriddenKeys += stats.overriddenEnKeys + stats.overriddenArKeys;

    const fileName = `${domain}.translations.json`;
    const filePath = path.join(SOURCES_DIR, fileName);

    try {
      // Sort keys for deterministic output
      const sortedBundle: TranslationBundle = {
        en: Object.fromEntries(
          Object.entries(bundle.en).sort(([a], [b]) => a.localeCompare(b)),
        ),
        ar: Object.fromEntries(
          Object.entries(bundle.ar).sort(([a], [b]) => a.localeCompare(b)),
        ),
      };

      // Validate before writing
      if (!validateBundle(domain, sortedBundle)) {
        console.error(`âŒ Validation failed for ${domain}`);
        process.exit(1);
      }

      fs.writeFileSync(
        filePath,
        JSON.stringify(sortedBundle, null, 2),
        "utf-8",
      );

      const enCount = Object.keys(sortedBundle.en).length;
      const arCount = Object.keys(sortedBundle.ar).length;

      totalEnKeys += enCount;
      totalArKeys += arCount;

      // Track parity issues
      if (enCount !== arCount) {
        parityIssues.push({
          domain,
          enCount,
          arCount,
          diff: enCount - arCount,
        });
      }

      const parityMarker = enCount === arCount ? "âœ“" : "âš ï¸";
      console.log(
        `  ${parityMarker} ${fileName.padEnd(40)} (${String(enCount).padStart(5)} en, ${String(arCount).padStart(5)} ar)`,
      );
    } catch (err) {
      console.error(`  âœ— Failed to write ${fileName}:`, err);
      process.exit(1);
    }
  }

  console.log(`\nâœ… Successfully flattened base dictionaries!`);
  console.log(`ðŸ“‚ Location: ${SOURCES_DIR}`);
  console.log(`ðŸ“Š Total: ${totalEnKeys} en keys, ${totalArKeys} ar keys`);

  // Data loss detection
  const originalEnCount = Object.keys(enFlat).length;
  const originalArCount = Object.keys(arFlat).length;
  const enLoss = originalEnCount - totalEnKeys;
  const arLoss = originalArCount - totalArKeys;

  if (enLoss !== 0 || arLoss !== 0) {
    console.log(`\nâš ï¸  DATA LOSS DETECTED:`);
    if (enLoss > 0)
      console.log(`    - Lost ${enLoss} English keys during grouping/merge`);
    if (enLoss < 0)
      console.log(
        `    - Gained ${-enLoss} English keys (duplicates or merge artifacts)`,
      );
    if (arLoss > 0)
      console.log(`    - Lost ${arLoss} Arabic keys during grouping/merge`);
    if (arLoss < 0)
      console.log(
        `    - Gained ${-arLoss} Arabic keys (duplicates or merge artifacts)`,
      );
  }

  // Report parity issues
  if (parityIssues.length > 0) {
    console.log(
      `\nâš ï¸  TRANSLATION PARITY ISSUES (${parityIssues.length} domains):`,
    );
    console.log(
      `\n    Domain                                    EN      AR      Diff`,
    );
    console.log(`    ${"=".repeat(70)}`);

    // Sort by absolute diff (largest first)
    parityIssues.sort((a, b) => Math.abs(b.diff) - Math.abs(a.diff));

    for (const issue of parityIssues.slice(0, 20)) {
      // Show top 20
      const diffStr = issue.diff > 0 ? `+${issue.diff}` : String(issue.diff);
      console.log(
        `    ${issue.domain.padEnd(40)} ${String(issue.enCount).padStart(6)} ${String(issue.arCount).padStart(6)} ${diffStr.padStart(8)}`,
      );
    }

    if (parityIssues.length > 20) {
      console.log(
        `    ... and ${parityIssues.length - 20} more domains with mismatches`,
      );
    }

    console.log(
      `\n    ðŸ’¡ Run: npx tsx scripts/check-translation-parity.ts for detailed report`,
    );
  } else {
    console.log(`\nâœ… All domains have matching key counts across locales`);
  }

  if (totalOverriddenKeys > 0) {
    console.log(
      `\nðŸ“ Merge Summary: ${totalOverriddenKeys} keys overridden with fresh values`,
    );
  }

  console.log(`\nðŸ“‹ Next steps:`);
  console.log(`  1. Run: pnpm i18n:build`);
  console.log(`  2. Test: pnpm tsc --noEmit`);
  console.log(`  3. Verify: npx tsx scripts/check-translation-parity.ts`);
  console.log(`  4. Commit modular sources and generated artifacts`);

  if (parityIssues.length > 0) {
    console.log(
      `\nâš ï¸  WARNING: Parity issues detected. Review before deploying.\n`,
    );
    process.exit(1);
  }

  console.log("");
}

// Run the flattening
writeDomainFiles();

]]>
</file>

</batch_content>
